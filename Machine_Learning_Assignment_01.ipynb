{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning\n",
        "\n",
        " 1. What is a parameter ?\n",
        "\n",
        "  - A parameter is like a special piece of information you give to a function or a process to help it work correctly. Think of it as an input that tells the function what to do or what data to use. For example, when you tell someone to bake a cake, you might specify the flavor or the size — those details are like parameters. In programming, parameters allow functions to be flexible and reusable because you can give them different values each time. Instead of writing many functions for similar tasks, you write one function with parameters, and it can handle different situations. Parameters make programs smarter and more efficient by letting us control how functions behave without changing their basic code. So, parameters are essential building blocks for customizing actions in both daily life and coding.\n",
        "\n",
        "\n",
        " 2. What is correlation?\n",
        "\n",
        " - Correlation is a way to understand how two things are connected or related to each other. Imagine you notice that when it rains more, people carry umbrellas more often — this is a correlation between rain and umbrella use. It doesn’t mean one causes the other, but they tend to happen together. In data and statistics, correlation measures how strongly two variables move together. If both increase or decrease together, it’s called a positive correlation. If one goes up while the other goes down, it’s a negative correlation. Correlation helps us find patterns and relationships in data, which can be very useful to predict or understand behavior. However, it’s important to remember that correlation doesn’t always mean one thing causes the other; they just happen to be related.\n",
        " Negative correlation means when one thing goes up, the other goes down. For example, if you watch more TV, your test scores might go down. This shows they move in opposite directions. It helps us understand how two things are related but in reverse. However, it doesn’t mean one causes the other, just that they change opposite to each other. Negative correlation helps find patterns in data and real life.\n",
        "\n",
        " 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "  - Machine Learning is a way for computers to learn from data and improve their performance without being explicitly programmed for every task. Instead of giving step-by-step instructions, we teach the computer by showing it examples, and it figures out patterns on its own. This helps machines make decisions or predictions based on new information. The main components of Machine Learning are data, algorithms, and models. Data is the information we use to teach the computer. Algorithms are the rules or methods the computer follows to learn from the data. Models are the results of this learning, which can then be used to make predictions or decisions. Together, these components help machines become smarter and more useful in solving real-world problems.\n",
        "\n",
        " 4. How does loss value help in determining whether the model is good or not?\n",
        "   \n",
        "   - Loss value is like a score that tells us how well a machine learning model is doing. When the model makes predictions, the loss value measures how far those predictions are from the actual correct answers. A small loss value means the model’s predictions are very close to the real results, so the model is doing a good job. On the other hand, a large loss value means the model’s predictions are often wrong or far off. By looking at the loss value during training, we can understand if the model is learning properly or if it needs improvement. Lowering the loss is the main goal when training a model, because it means the model is becoming more accurate and reliable for making future predictions.\n",
        "\n",
        " 5. What are continuous and categorical variables?\n",
        "\n",
        "   - Continuous and categorical variables are two types of data we use to understand information. Continuous variables are numbers that can take any value within a range. For example, height, weight, or temperature can be 150.5 cm, 70.2 kg, or 36.6 degrees — they can have decimals and many possible values. Categorical variables, on the other hand, are categories or groups. They describe qualities or types, like colors (red, blue, green), types of fruit (apple, banana), or yes/no answers. You can’t measure categorical variables with numbers in a meaningful way. Understanding the difference helps us choose the right methods to analyze data, because continuous data and categorical data are treated differently in statistics and machine learning.\n",
        "\n",
        " 6. How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?\n",
        "\n",
        "   - In Machine Learning, categorical variables need to be converted into numbers because computers can only work with numbers. To handle these variables, we use special techniques. One common method is One-Hot Encoding, where each category is turned into a new column with values 0 or 1, showing if the category is present or not. Another method is Label Encoding, where each category is given a unique number, like 1, 2, or 3. Sometimes, for categories with many options, techniques like Target Encoding or Frequency Encoding are used, which replace categories with statistics based on the data. Choosing the right technique depends on the problem and the machine learning model. Handling categorical variables properly helps models understand the data better and make more accurate predictions.\n",
        "\n",
        " 7. What do you mean by training and testing a dataset?\n",
        "\n",
        "  - Training and testing a dataset are two important steps in building a machine learning model. When we train a model, we give it a set of data called the training dataset. This data includes both the input and the correct answers, so the model can learn patterns and relationships from it. Think of it like studying for an exam by practicing with sample questions and answers. After the model has learned from the training data, we use a different set called the testing dataset to see how well the model performs on new, unseen data. This is like taking a real test to check how much you’ve learned. Testing helps us understand if the model can make good predictions in real life, not just on the data it was trained on. Splitting data into training and testing ensures the model is accurate and reliable.\n",
        "\n",
        " 8. What is sklearn.preprocessing?\n",
        "\n",
        "  - sklearn.preprocessing is a part of the popular Python library called scikit-learn, which helps prepare data before using it in machine learning models. This tool provides easy ways to transform or change your data so that the model can understand it better. For example, it can scale numbers to a common range, convert categories into numbers, or handle missing values. Just like washing and chopping vegetables before cooking, sklearn.preprocessing cleans and prepares your data to make the machine learning process smoother and more effective. Using these preprocessing steps often improves how well a model learns and predicts, making it an important step in any machine learning project.\n",
        "\n",
        " 9. What is a Test set?\n",
        "\n",
        "  - A test set is a part of the data we keep aside when building a machine learning model. After the model learns from the training data, the test set is used to check how well the model performs on new, unseen information. It’s like a final exam to see if the model really understands the patterns and can make good predictions outside of what it has already learned. Using a test set helps us avoid tricks or guessing in the learning process and gives a fair idea of how the model will work in real life. Without a test set, we wouldn’t know if the model is reliable or just memorizing the training data. So, the test set is very important to measure a model’s true accuracy.\n",
        "\n",
        " 10. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "  -  In Python, we split data for model fitting into training and testing sets to help the machine learning model learn and then check how well it performs. We usually use a tool called train_test_split from the sklearn.model_selection module to do this easily. This function takes your full dataset and divides it into two parts: one for training and one for testing. You can decide how much data goes into each part, like 70% for training and 30% for testing. This helps the model learn from most of the data and then test itself on new data it hasn’t seen before. Splitting the data this way makes sure the model doesn’t just memorize the answers but can actually make good predictions on real, unseen data. It’s a simple but very important step in machine learning.\n",
        "\n",
        " 11. Why do we have to perform EDA before fitting a model to the data?\n",
        "  \n",
        "   - We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the data better. EDA is like getting to know your ingredients before cooking — you check their quality, type, and amount. In data, this means looking for missing values, errors, patterns, or unusual points that might affect the model. By doing EDA, we can decide how to clean the data, what features are important, and how to handle problems like outliers or biased data. This step helps make sure the model learns from good and meaningful information. Without EDA, the model might give wrong or unreliable results because it could be confused by messy or strange data. So, EDA is important to prepare the data and build a smarter, more accurate model.\n",
        "\n",
        " 12. What is correlation ?\n",
        "\n",
        "  - Correlation is a way to measure how two things are related to each other. It tells us whether they move together or not. For example, if when one thing increases, the other also increases, they have a positive correlation. If one goes up while the other goes down, they have a negative correlation. Correlation helps us understand relationships between variables, like how studying time might relate to exam scores. However, correlation doesn’t mean one thing causes the other; it just shows that they happen to change together in some way\n",
        "\n",
        " 13. What does negative correlation mean ?\n",
        "\n",
        "  - Negative correlation means that two things move in opposite directions. When one thing increases, the other decreases, and when one goes down, the other goes up. For example, if the number of hours spent watching TV goes up, a student’s study time might go down. This shows a negative correlation because as one goes up, the other goes down. Negative correlation helps us understand situations where two factors affect each other in opposite ways. However, it doesn’t mean one causes the other; it just shows they change in reverse to each other.\n",
        "\n",
        " 14. How can you find correlation between variables in Python ?\n",
        "\n",
        "  - To find correlation between variables in Python, you usually use libraries like pandas or numpy. The easiest way is with pandas’ corr() function. First, you load your data into a pandas DataFrame, then call .corr() on it. This calculates the correlation between all pairs of numeric columns and shows a table of correlation values. For example, if you have columns like “age” and “income,” .corr() will tell you how strongly they are related, with values between -1 (perfect negative correlation) and 1 (perfect positive correlation). You can also calculate correlation between two specific variables using df['col1'].corr(df['col2']). This helps you quickly understand relationships in your data and decide which variables to focus on\n",
        "\n",
        " 15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "  - Causation means that one event directly causes another event to happen. For example, if you study hard, you are more likely to get good grades — studying causes better results. Correlation, however, means two things happen together or have a relationship, but one doesn’t necessarily cause the other. For instance, ice cream sales and drowning cases both increase during summer. They are correlated because they happen at the same time, but buying ice cream doesn’t cause drowning. The main difference is that causation shows a direct cause and effect, while correlation only shows a connection or pattern without proving one causes the other. Understanding this difference is important so we don’t mistake coincidence for real cause.\n",
        "\n",
        " 16. What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
        "\n",
        "  - An optimizer is a tool in machine learning that helps a model learn by adjusting its settings to reduce mistakes and improve accuracy. It works like a guide, showing the model how to get better step by step. There are different types of optimizers. Gradient Descent moves slowly in the direction that lowers the error, like walking downhill to find the lowest point. Stochastic Gradient Descent (SGD) is faster because it updates using small parts of data instead of all at once, but it can be less smooth. Adam is a popular optimizer that automatically adjusts the learning speed for each part of the model, making learning efficient and easier to manage. Each optimizer helps the model improve in different ways to make better predictions.\n",
        "\n",
        " 17. What is sklearn.linear_model ?\n",
        "\n",
        "  - sklearn.linear_model is a part of the scikit-learn library in Python that provides tools to create and use linear models for machine learning. Linear models try to find a straight-line relationship between input features and the output. This module includes popular algorithms like Linear Regression for predicting continuous values, Logistic Regression for classification tasks, and others like Ridge and Lasso regression that add extra rules to make the model better. Using sklearn.linear_model, you can easily build models to predict outcomes based on data, such as predicting house prices or classifying emails as spam or not. It’s a simple but powerful tool for many machine learning problems.\n",
        "\n",
        " 18. What does model.fit() do? What arguments must be given ?\n",
        "\n",
        "  -  The model.fit() function is a fundamental method in machine learning used to train a model on a given dataset. When you call model.fit(), you're essentially telling the model to learn from the data you provide, adjusting its internal parameters to minimize the error between its predictions and the actual values. This process is known as \"fitting\" or \"training\" the model.\n",
        "\n",
        "  To use model.fit(), you typically need to provide at least two main arguments: the training data (usually denoted as X) and the corresponding target values (usually denoted as y). Additionally, you can specify other parameters such as the number of epochs (iterations over the entire dataset), batch size (number of samples per gradient update), and validation data (a separate dataset used to evaluate the model's performance during training).\n",
        "\n",
        "\n",
        " 19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "  - The model.predict() function is used when you want your machine learning model to guess or estimate something after it has been trained. You give it some input data (just like the data it saw during training), and it gives you a result — like predicting a price, a category, or a number. For example, if you trained a model to predict house prices, you can give it details like size and location, and it will predict the price. The only thing you must give to model.predict() is the input data. This data should be in the same format as what the model was trained with, usually as a list, array, or table.\n",
        "\n",
        " 20. What are continuous and categorical variables ?\n",
        "\n",
        "  - In data, we often deal with two main types of variables: continuous and categorical. Continuous variables are numbers that can take any value within a range. For example, height, weight, temperature, or price — these can have decimal values and can keep changing smoothly. On the other hand, categorical variables are used to represent different groups or categories. These are not measured with numbers but with labels, like gender (male/female), color (red/blue/green), or city names. In short, continuous variables are about \"how much,\" while categorical variables are about \"which type .\n",
        "\n",
        " 21. What is feature scaling? How does it help in Machine Learning?\n",
        "  \n",
        "  - Feature scaling is a technique used in machine learning to make sure all the input features (columns) in your data are on a similar scale. Sometimes, your data may have values like age (20–60) and salary (20,000–1,00,000). Since the scales are very different, some algorithms (like K-Nearest Neighbors, Gradient Descent, or SVM) may give more importance to the bigger numbers, which can reduce accuracy. Feature scaling adjusts the values — usually by normalizing or standardizing — so they fit within the same range (like 0 to 1 or have a mean of 0 and standard deviation of 1). This helps the model learn better, train faster, and make more accurate predictions.\n",
        "\n",
        " 22. How do we perform scaling in Python ?\n",
        "\n",
        " - In Python, we perform feature scaling using libraries like scikit-learn. The most common methods are Standardization and Normalization. Standardization uses StandardScaler(), which scales the data so it has a mean of 0 and a standard deviation of 1. Normalization uses MinMaxScaler(), which brings all values between 0 and 1. To use them, you first import the scaler, fit it on your data, and then transform the data. For example, with StandardScaler(), you write: scaler = StandardScaler(), then scaler.fit(X) and X_scaled = scaler.transform(X). This process ensures all your features are on the same scale, which is especially helpful for machine learning algorithms that are sensitive to differences in magnitude. It makes the model learn more effectively and improves accuracy. Always scale after splitting the data into training and test sets to avoid data leakage.\n",
        "\n",
        " 23. What is sklearn.preprocessing?\n",
        "\n",
        "  - sklearn.preprocessing is a part of the scikit-learn library in Python that helps you prepare your data before feeding it into a machine learning model. Think of it like getting your ingredients ready before cooking. Many times, raw data isn’t clean — numbers may be too big or small, some values may be missing, or text labels need to be turned into numbers. This module gives you tools to fix all that. For example, it can scale all your numbers to a similar range, convert words like \"yes\" and \"no\" into 1s and 0s, or fill in missing data. By using sklearn.preprocessing, you make sure your data is in the best shape, so your model can learn properly and give better results.\n",
        "\n",
        " 24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "  - In Python, we split our data into training and testing sets using a function called train_test_split from the sklearn.model_selection module. This step is important because we want the model to learn from one part of the data (training set) and then test how well it performs on new, unseen data (testing set). It’s like preparing for an exam — you study using some material, and then test your knowledge using different questions. The function allows you to decide what percentage of data should be used for training and what percentage for testing. This helps in checking if the model is truly learning or just memorizing the training data. A well-split dataset leads to better model performance and more trustworthy\n",
        "\n",
        " 25. Explain data encoding ?\n",
        "\n",
        "   - In machine learning, computers work with numbers — they don’t understand words or categories like humans do. That’s where data encoding comes in. Data encoding is the process of converting text or category values into numbers so that a machine learning model can understand and work with them. For example, if we have a column for \"Gender\" with values like \"Male\" and \"Female\", the model won’t understand these directly. So, we convert them into numbers, like 0 for Male and 1 for Female. This is called Label Encoding. But sometimes, just giving numbers like 1, 2, 3 to categories can confuse the model — it might think category 3 is bigger or better than category 1. To avoid that, we use One-Hot Encoding, which creates a separate column for each category and marks it as 1 (true) or 0 (false). So, if you have three fruits — Apple, Banana, and Mango — One-Hot Encoding will create three columns: Is_Apple, Is_Banana, and Is_Mango. Each row will have a 1 in the right column and 0 in the others. This way, the model treats each category equally. Data encoding is a small but very important step in preparing your data. Without it, many machine learning models simply won’t work because they can’t process text directly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   ----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "3OmBtLK1yuPV"
      }
    }
  ]
}